{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before your start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the data \n",
    "# spent 5 min check the data\n",
    "# spend 5 min plan out the things you want to do. \n",
    "# if its supervised, spent time on checking the labels \n",
    "## is the imbalanced data? -- stratify\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('transaction_data/user-0.csv',encoding='latin-1', header=0)\n",
    "# header = 0 means the first role is the index\n",
    "# header = None means no header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and combine all the files togehter\n",
    "path = r'transaction_data/' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the data\n",
    "df.info()\n",
    "df.describe()\n",
    "df.columns\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.auth_id.nunique() # number of unique values\n",
    "df.Vendor.unique()   # display the unique ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is duplicate ID in the dataset\n",
    "df.auth_id.value_counts() # 查看是否有重复， 每条ID有几条data\n",
    "# different between value_counts() and counts()\n",
    "df.auth_id.counts() # 看非nan的entry的个数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # 看有多少missing value\n",
    "df.info() \n",
    "\n",
    "df[df=='?'] = np.nan # 将问号都写成nan\n",
    "\n",
    "## fill missing values\n",
    "df = df.fillna(np.nan) # fill 成nan\n",
    "dff.fillna(dff.mean()['B']) #将B coloumn 的missing 填写成他的mean\n",
    "\n",
    "cols = ['a','b','c'] # 将missing value 填充成high frequency value\n",
    "for col in cols:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing the dataframe\n",
    "df.iloc[:3] # 选取前三行\n",
    "df.iloc[:3][['Date','Amount']] # 选取前三行的2个column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change categorical value to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1:\n",
    "df['gender'] = df.gender.map(lambda x: 1 if x=='male' else 0)\n",
    "# method 2:\n",
    "df['income'] = df.income.map({'<=50K': 0, '>50K': 1})\n",
    "#method 3:\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "dataset['occupation.num'] = le.fit_transform(dataset['occupation'])\n",
    "# method 4: get dummies, 000100 one hot encoding\n",
    "vendor_train = pd.get_dummies(train_data['vendor_id'], prefix='vi', prefix_sep='_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create bins for numerical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins 20-29, 30-39, 40-49, n bins, n-1 labels\n",
    "df_ages['age_by_decade'] = pd.cut(x=df_ages['age'], bins=[20, 29, 39, 49], labels=['20s', '30s', '40s']) \n",
    "pd.cut(x=dataset['age'], bins=[0, 19, 29, 39, 49, 59, 69,100], labels=['below20s', '20s', '30s', '40s', '50s', '60s', 'above60s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time stamps manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date']) # change a column from string format to datetime format\n",
    "df['Date'].diff() # take the different of date on the column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract date information .dt.day, .dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date information\n",
    "train_data['day_of_month'] = train_data.pickup_datetime.dt.day\n",
    "train_data['month'] = train_data.pickup_datetime.dt.month\n",
    "train_data['hour_of_day'] = train_data.pickup_datetime.dt.hour\n",
    "train_data['weekday'] = train_data.pickup_datetime.dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change series to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join merge concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1, df2, on = 'colA')\n",
    "new_df = new_df.merge(temp_df, left_index=True, right_index=True)\n",
    "\n",
    "# concat the columns\n",
    "pieces = [df[:3], df[3:7], df[7:]]\n",
    "pd.concat(pieces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort values and sort index\n",
    "\n",
    "df.sort_values(by='B')\n",
    "df.sort_index(axis=1, ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupby examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see unique values for each group\n",
    "x = df.groupby('auth_id').hobby.unique()\n",
    "x.index # is the indexes for each group\n",
    "x.loc[ind] # give you the entry to each value of group with index ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration with groupby\n",
    "# Group the dataframe by regiment, and for each regiment,\n",
    "for name, group in df.groupby('regiment'): \n",
    "    # print the name of the regiment\n",
    "    print(name)\n",
    "    # print the data of that regiment\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 拿出其中一个组\n",
    "df.groupby('auth_id').get_group(624)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个更加复杂点的example\n",
    "# https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_groups/\n",
    "def get_stats(group):\n",
    "    return {'min': group.min(), 'max': group.max(), 'count': group.count(), 'mean': group.mean()}\n",
    "bins = [0, 25, 50, 75, 100]\n",
    "group_names = ['Low', 'Okay', 'Good', 'Great']\n",
    "df['categories'] = pd.cut(df['postTestScore'], bins, labels=group_names)\n",
    "df['postTestScore'].groupby(df['categories']).apply(get_stats).unstack() # apply的这个function 是针对每个group的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply to rows\n",
    "df.apply(lambda row: f(row), axis=1)\n",
    "def f(row):\n",
    "    row['A']*row['B']\n",
    "    \n",
    "## apply to elements:\n",
    "df['A'].apply(lambda x: x if x>=0 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### historgram\n",
    "sns.distplot(total_spending, bins=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### barplot\n",
    "\n",
    "### x axis is category, y is value\n",
    "sns.barplot(x =new_df.index, y= -new_df.sport_spending) # 这个是简单plot\n",
    "ax = sns.violinplot(x='income', y=\"hours.per.week\", data=dataset, palette=\"muted\", split=True) # 这个可以看distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### show two plots on the the same page\n",
    "fig, ((a,b)) = plt.subplots(1,2,figsize=(15,8))\n",
    "plt.xticks(rotation=45)\n",
    "sns.distplot(dataset[\"fnlwgt\"][dataset.income==0], ax=a)\n",
    "sns.distplot(dataset[\"fnlwgt\"][dataset.income==1], ax=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countplot\n",
    "### x is category and y is also category\n",
    "fig, ((a,b),(c,d),(e,f)) = plt.subplots(3,2,figsize=(15,20))\n",
    "plt.xticks(rotation=45)\n",
    "sns.countplot(dataset['workclass'],hue=dataset['income'],ax=f)\n",
    "sns.countplot(dataset['relationship'],hue=dataset['income'],ax=b)\n",
    "sns.countplot(dataset['marital.status'],hue=dataset['income'],ax=c)\n",
    "sns.countplot(dataset['race'],hue=dataset['income'],ax=d)\n",
    "sns.countplot(dataset['sex'],hue=dataset['income'],ax=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a correlation plot for numerical features\n",
    "numeric_features = ['a','b','c']\n",
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "sns.heatmap(dataset[numeric_features].corr(), annot=True, fmt ='.2f')\n",
    "ax.set_ylim(len(numeric_features)-0.2, -0.2) # heatmap visualization issue quick fix \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.20 # 20% of data is used for validation\n",
    "seed = 1\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X,Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify the propotion of the label is balanced\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=123, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(df, groups=df['Group_Id']))\n",
    "train = df.iloc[train_inds]\n",
    "test = df.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest is the best out of all\n",
    "random_forest = RandomForestClassifier(n_estimators=250,max_features=5)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "predictions = random_forest.predict(X_validation)\n",
    "print(\"Accuracy: %s%%\" % (100*accuracy_score(Y_validation, predictions)))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance from random forest\n",
    "importances = random_forest.feature_importances_\n",
    "sorted_idx = importances.argsort()\n",
    "feature_names = feature[:13]\n",
    "feature_names0 = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "y_ticks = np.arange(0, len(sorted_idx))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, importances[sorted_idx])\n",
    "ax.set_yticklabels(feature_names0)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF features\n",
    "n_estimators = number of trees in the foreset\n",
    "max_features = max number of features considered for splitting a node\n",
    "max_depth = max number of levels in each decision tree\n",
    "min_samples_split = min number of data points placed in a node before the node is split\n",
    "min_samples_leaf = min number of data points allowed in a leaf node\n",
    "bootstrap = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
