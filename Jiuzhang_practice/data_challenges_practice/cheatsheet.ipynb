{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before your start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the data \n",
    "# spent 5 min check the data\n",
    "# spend 5 min plan out the things you want to do. \n",
    "# if its supervised, spent time on checking the labels \n",
    "## is the imbalanced data? -- stratify\n",
    "\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import lightgbm as lgb\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report, roc_auc_score,roc_curve, auc\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import itertools \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('transaction_data/user-0.csv',encoding='latin-1', header=0)\n",
    "# header = 0 means the first role is the index\n",
    "# header = None means no header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and combine all the files togehter\n",
    "path = r'transaction_data/' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the data\n",
    "df.info()\n",
    "df.describe()\n",
    "df.columns\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.auth_id.nunique() # number of unique values\n",
    "df.Vendor.unique()   # display the unique ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is duplicate ID in the dataset\n",
    "df.auth_id.value_counts() # 查看是否有重复， 每条ID有几条data\n",
    "# different between value_counts() and counts()\n",
    "df.auth_id.counts() # 看非nan的entry的个数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # 看有多少missing value\n",
    "df.info() \n",
    "\n",
    "df[df=='?'] = np.nan # 将问号都写成nan\n",
    "## fill missing values\n",
    "df = df.fillna(np.nan) # fill 成nan\n",
    "dff.fillna(dff.mean()['B']) #将B coloumn 的missing 填写成他的mean\n",
    "\n",
    "cols = ['a','b','c'] # 将missing value 填充成high frequency value\n",
    "for col in cols:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing the dataframe\n",
    "df.iloc[:3] # 选取前三行\n",
    "df.iloc[:3][['Date','Amount']] # 选取前三行的2个column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding duplicated rows\n",
    "\n",
    "sum(df_info.duplicated(['credit_card'])) # there is no duplicates rows for ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change categorical value to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a58138b5e730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# method 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'male'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# method 2:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'income'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'<=50K'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'>50K'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# method 1:\n",
    "df['gender'] = df.gender.map(lambda x: 1 if x=='male' else 0)\n",
    "\n",
    "# method 2:\n",
    "df['income'] = df.income.map({'<=50K': 0, '>50K': 1})\n",
    "\n",
    "#method 3:\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "dataset['occupation.num'] = le.fit_transform(dataset['occupation'])\n",
    "# or\n",
    "category_data = ['a','b','c']\n",
    "for col in category_data:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "\n",
    "# method 4: get dummies, 000100 one hot encoding\n",
    "vendor_train = pd.get_dummies(train_data['vendor_id'], prefix='vi', prefix_sep='_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### multi_index \n",
    "df.reset_index()  # turn multi-index to flat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rename column\n",
    "df.rename(columns={'a':'b'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create new dataframe with multiple column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'ID': res.index.get_level_values(0), 'month': res.index.get_level_values(1), 'spending': res.values}  \n",
    "new_df = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create bins for numerical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins 20-29, 30-39, 40-49, n bins, n-1 labels\n",
    "df_ages['age_by_decade'] = pd.cut(x=df_ages['age'], bins=[20, 29, 39, 49], labels=['20s', '30s', '40s']) \n",
    "pd.cut(x=dataset['age'], bins=[0, 19, 29, 39, 49, 59, 69,100], labels=['below20s', '20s', '30s', '40s', '50s', '60s', 'above60s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time stamps manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date']) # change a column from string format to datetime format\n",
    "df['Date'].diff() # take the different of date on the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract date information .dt.day, .dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date information\n",
    "train_data['day_of_month'] = train_data.pickup_datetime.dt.day\n",
    "train_data['month'] = train_data.pickup_datetime.dt.month\n",
    "train_data['hour_of_day'] = train_data.pickup_datetime.dt.hour\n",
    "train_data['weekday'] = train_data.pickup_datetime.dt.weekday\n",
    "\n",
    "# method 2\n",
    "data['month'] = data['date'].apply(lambda x: x.month)\n",
    "data['week'] = data['date'].apply(lambda x: x.week) # 0-51\n",
    "data['dayofweek'] = data['date'].apply(lambda x: x.dayofweek)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change series to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join merge concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1, df2, on = 'colA')\n",
    "new_df = new_df.merge(temp_df, left_on='Time', right_on='Time') #这个比较好使\n",
    "new_df = new_df.merge(temp_df, left_index=True, right_index=True)\n",
    "\n",
    "# concat the columns\n",
    "pieces = [df[:3], df[3:7], df[7:]]\n",
    "pd.concat(pieces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort values and sort index\n",
    "\n",
    "df.sort_values(by='B')\n",
    "df.sort_index(axis=1, ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupby examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see unique values for each group\n",
    "x = df.groupby('auth_id').hobby.unique()\n",
    "x.index # is the indexes for each group\n",
    "x.loc[ind] # give you the entry to each value of group with index ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration with groupby\n",
    "# Group the dataframe by regiment, and for each regiment,\n",
    "for name, group in df.groupby('regiment'): \n",
    "    # print the name of the regiment\n",
    "    print(name)\n",
    "    # print the data of that regiment\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 拿出其中一个组\n",
    "df.groupby('auth_id').get_group(624)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert groupby result into dataframe， \n",
    "df.groupby('device_id').user_id.count().reset_index() # reset_index() 在这里就是用来将它转化成为data frame\n",
    "\n",
    "# 类似操作， 相当好使。\n",
    "median_amount = df_trans.groupby(['credit_card']).transaction_dollar_amount.median()\n",
    "median_values = median_amount.reset_index()\n",
    "median_values.rename(columns={'transaction_dollar_amount':'median_amount'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个更加复杂点的example\n",
    "# https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_groups/\n",
    "def get_stats(group):\n",
    "    return {'min': group.min(), 'max': group.max(), 'count': group.count(), 'mean': group.mean()}\n",
    "bins = [0, 25, 50, 75, 100]\n",
    "group_names = ['Low', 'Okay', 'Good', 'Great']\n",
    "df['categories'] = pd.cut(df['postTestScore'], bins, labels=group_names)\n",
    "df['postTestScore'].groupby(df['categories']).apply(get_stats).unstack() # apply的这个function 是针对每个group的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply to rows\n",
    "df.apply(lambda row: f(row), axis=1)\n",
    "def f(row):\n",
    "    row['A']*row['B']\n",
    "    \n",
    "## apply to elements:\n",
    "df['A'].apply(lambda x: x if x>=0 else 0)\n",
    "\n",
    "## below is one example. \n",
    "country = data['ip_address'].apply(lambda x: f(x))\n",
    "def f(x):\n",
    "    # use boolean indexing for selection\n",
    "    tmp = address2country[(x>=address2country.lower_bound_ip_address) & (x<= address2country.upper_bound_ip_address)] \n",
    "    if len(tmp)==1:\n",
    "        return tmp.country.values[0]\n",
    "    else:\n",
    "        return 'NA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorization to calculate the distance value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a basic Haversine distance formula\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    MILES = 3959\n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    total_miles = MILES * c\n",
    "    return total_miles\n",
    "distance = haversine(df_trans['Long'],df_trans['Lat'],df_trans['medianLong'],df_trans['medianLat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### historgram\n",
    "sns.distplot(total_spending, bins=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### barplot\n",
    "\n",
    "### x axis is category, y is value\n",
    "sns.barplot(x =new_df.index, y= -new_df.sport_spending) # 这个是简单plot\n",
    "ax = sns.violinplot(x='income', y=\"hours.per.week\", data=dataset, palette=\"muted\", split=True) # 这个可以看distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### show two plots on the the same page\n",
    "fig, ((a,b)) = plt.subplots(1,2,figsize=(15,8))\n",
    "plt.xticks(rotation=45)\n",
    "sns.distplot(dataset[\"fnlwgt\"][dataset.income==0], ax=a)\n",
    "sns.distplot(dataset[\"fnlwgt\"][dataset.income==1], ax=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### countplot\n",
    "### x is category and y is also category\n",
    "fig, ((a,b),(c,d),(e,f)) = plt.subplots(3,2,figsize=(15,20))\n",
    "plt.xticks(rotation=45)\n",
    "sns.countplot(dataset['workclass'],hue=dataset['income'],ax=f)\n",
    "sns.countplot(dataset['relationship'],hue=dataset['income'],ax=b)\n",
    "sns.countplot(dataset['marital.status'],hue=dataset['income'],ax=c)\n",
    "sns.countplot(dataset['race'],hue=dataset['income'],ax=d)\n",
    "sns.countplot(dataset['sex'],hue=dataset['income'],ax=e)\n",
    "\n",
    "# another way to do countplot\n",
    "# loan_repaid is label, age_group is category value\n",
    "# left side is the count of each age group, right side is the ratio for loan_repaid rate\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "sns.countplot(x='age_group', data=df, ax=ax[0])\n",
    "ax[0].set_xlabel('age_group', fontsize=12)\n",
    "ax[0].set_ylabel('Count', fontsize=12)\n",
    "ax[0].set_title('Count Plot of age', fontsize=16)\n",
    "\n",
    "sns.barplot(x='age_group', y='loan_repaid', data=df, ax=ax[1])\n",
    "ax[1].set_xlabel('age_group', fontsize=12)\n",
    "ax[1].set_ylabel('Loan Repaid Ratio', fontsize=12)\n",
    "ax[1].set_title('Loan Repaid Ratio vs. age', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a correlation plot for numerical features\n",
    "numeric_features = ['a','b','c']\n",
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "sns.heatmap(dataset[numeric_features].corr(), annot=True, fmt ='.2f')\n",
    "ax.set_ylim(len(numeric_features)-0.2, -0.2) # heatmap visualization issue quick fix \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is numerical, y is categories, can use facet plot\n",
    "g = sns.FacetGrid(data, col=\"class\", col_order=[0, 1])\n",
    "g = g.map(plt.hist, \"timediff\", bins=100, color=\"m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['user_id', 'signup_time','purchase_time','class'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_size = 0.20 # 20% of data is used for validation\n",
    "seed = 1\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X,Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify the propotion of the label is balanced\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=123, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(df, groups=df['Group_Id']))\n",
    "train = df.iloc[train_inds]\n",
    "test = df.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest is the best out of all\n",
    "random_forest = RandomForestClassifier(n_estimators=250,max_features=5)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "predictions = random_forest.predict(X_validation)\n",
    "print(\"Accuracy: %s%%\" % (100*accuracy_score(Y_validation, predictions)))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance from random forest\n",
    "importances = random_forest.feature_importances_\n",
    "sorted_idx = importances.argsort()\n",
    "feature_names = feature[:13]\n",
    "feature_names0 = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "y_ticks = np.arange(0, len(sorted_idx))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, importances[sorted_idx])\n",
    "ax.set_yticklabels(feature_names0)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF features\n",
    "n_estimators = number of trees in the foreset\n",
    "max_features = max number of features considered for splitting a node\n",
    "max_depth = max number of levels in each decision tree\n",
    "min_samples_split = min number of data points placed in a node before the node is split\n",
    "min_samples_leaf = min number of data points allowed in a leaf node\n",
    "bootstrap = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  xgboost\n",
    "# Now lets try use xgboost\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "dvalid = xgb.DMatrix(X_validation, label=Y_validation)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "\n",
    "xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n",
    "            'subsample': 0.8, 'lambda': 1.0, 'nthread': 2, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'objective': 'reg:linear','eval_metric': 'error'} # eval_metric by default is error\n",
    "model = xgb.train(xgb_pars, dtrain, 1000, watchlist, early_stopping_rounds=50,\n",
    "      maximize=False, verbose_eval=1)\n",
    "print('Modeling error %.5f' % model.best_score)\n",
    "\n",
    "xgb.plot_importance(model, max_num_features=28, height=0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two classes ROC curve\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fpr, tpr, _ = roc_curve(Y_validation, predictions) # Y_validation is golden label, predictions is the probability \n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "colors = itertools.cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "plt.plot(fpr, tpr, color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下这个方法对于multiclass适用\n",
    "import itertools\n",
    "# calculate roc auc\n",
    "num_labels = 2 # class 的个数\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_labels):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_validation, predictions)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_validation.ravel(), np.array(predictions).ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_labels)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_labels):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_labels\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = itertools.cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_labels), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### precision and recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-edd2b791bb4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Y_validation is goldern, y_pred is predictin prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'recall: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_validation' is not defined"
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_validation, y_pred) # Y_validation is goldern, y_pred is predictin prob\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "plt.plot(recall, precision, marker='.', label='xx')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
